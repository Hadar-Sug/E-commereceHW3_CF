{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import scipy.sparse as sp\n",
    "import sklearn.preprocessing\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "df = pd.read_csv('user_song.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_users = df.iloc[:,0].unique().tolist()\n",
    "train_songs = df.iloc[:,1].unique().tolist()\n",
    "train_couples = list(zip(df.iloc[:,0], df.iloc[:,1]))\n",
    "\n",
    "test_users = test_df.iloc[:,0].unique().tolist()\n",
    "test_songs = test_df.iloc[:,1].unique().tolist()\n",
    "test_couples = list(zip(test_df.iloc[:,0], test_df.iloc[:,1]))\n",
    "\n",
    "all_users = list(set(train_users + test_users))\n",
    "all_songs = list(set(train_songs + test_songs))\n",
    "all_couples = list(itertools.product(all_users, all_songs))\n",
    "new_couples = list(set(all_couples) - set(train_couples) - set(test_couples))\n",
    "# new_couples = random.sample(new_couples, k=int(len(new_couples) * 0.01)) # taking a fraction of zeros\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1 - Least Squares"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "data": {
      "text/plain": "1.8880274258705607"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = df.shape[0]\n",
    "m = n + len(new_couples)\n",
    "avg = df.iloc[:,2].mean()*(n/m)\n",
    "avg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "(16548, 1892)"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = len(all_users)  # Number of users\n",
    "num_songs = len(all_songs)  # Number of songs\n",
    "num_songs, num_users"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "# user dictionary\n",
    "users_hash = {}\n",
    "for i,user in enumerate(all_users):\n",
    "    users_hash[user] = i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "# song dictionary\n",
    "songs_hash = {}\n",
    "for j,song in enumerate(all_songs):\n",
    "    songs_hash[song] = j + num_users"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create the mat for LS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "train_users,train_songs =df['user_id'].map(users_hash).values, df['song_id'].map(songs_hash).values\n",
    "merged_train = np.column_stack((train_users,train_songs)).reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "rest_users, rest_items = zip(*new_couples)\n",
    "users_list=[users_hash[user] for user in rest_users]\n",
    "items_list=[songs_hash[song] for song in rest_items]\n",
    "merged_rest = np.column_stack((users_list,items_list)).reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "A_cols = np.concatenate((merged_train,merged_rest))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "rows1,rows2 = np.arange(int(0.5*len(A_cols))),np.arange(int(0.5*len(A_cols)))\n",
    "A_rows = np.column_stack((rows1,rows2)).reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "values = np.ones(len(A_rows))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "(62611832, 62611832, 62611832)"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(A_cols),len(A_rows),len(values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix,csr_matrix,csc_matrix\n",
    "\n",
    "A = coo_matrix((values, (A_rows, A_cols)),\\\n",
    "                    shape=(int(0.5*len(A_cols)),len(all_users)+len(all_songs))).tocsr()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create c for LS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "c_train = df['weight'].values - avg\n",
    "c_rest = np.full(len(new_couples),-avg)\n",
    "c = np.concatenate((c_train,c_rest))\n",
    "# type(b_train), type(b_rest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ls"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import lsqr\n",
    "final_b = lsqr(A=A,b=c)\n",
    "final_b = final_b[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## predictions - part 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "b_u_hat = final_b[:num_users]\n",
    "b_i_hat = final_b[num_users:]\n",
    "user_biases_mat = np.repeat(b_u_hat, num_songs).reshape(\n",
    "    (num_users, num_songs))  # duplicate to rows\n",
    "item_biases_mat = np.tile(b_i_hat, num_users).reshape(\n",
    "    (num_users, num_songs))\n",
    "avg_mat = np.full((num_users, num_songs), avg)\n",
    "predictions = avg_mat + user_biases_mat + item_biases_mat\n",
    "predictions_part1 = np.maximum(predictions,0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## true matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "songs_hash = {}\n",
    "for j, song in enumerate(all_songs):\n",
    "    songs_hash[song] = j"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "values = df['weight'].values\n",
    "rows = df['user_id'].map(users_hash).values\n",
    "cols = df['song_id'].map(songs_hash).values\n",
    "true_mat = coo_matrix((values, (rows, cols)), shape=predictions.shape).tocsr()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "test_users_mapped = test_df['user_id'].map(users_hash).values\n",
    "test_songs_mapped = test_df['song_id'].map(songs_hash).values\n",
    "test_indxs_mapped = list((zip(test_users_mapped,test_songs_mapped)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "score_mat = true_mat-predictions\n",
    "for idx in test_indxs_mapped:\n",
    "    score_mat[idx] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train scores - part 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "def get_score(true,pred,N):\n",
    "    score_matrix =true-pred\n",
    "    for indx in test_indxs_mapped:\n",
    "        score_mat[indx] = 0\n",
    "    return {'SSE': la.norm(score_matrix)**2, 'RMSE':((la.norm(score_matrix) ** 2) / N) ** 0.5}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE in Part 1 on train set: 187.47494295861532\n",
      "SSE in Part 1 on train set: 1100300951732.863\n"
     ]
    }
   ],
   "source": [
    "N = num_users*num_songs - len(test_indxs_mapped)\n",
    "scores_part1 = get_score(true_mat, predictions_part1, N)\n",
    "print(f'RMSE in Part 1 on train set: {scores_part1[\"RMSE\"]}\\n'\n",
    "      f'SSE in Part 1 on train set: {scores_part1[\"SSE\"]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Prediction - part 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_pred_1 = []\n",
    "for i, row in test_df.iterrows():\n",
    "    user = row['user_id']\n",
    "    song = row['song_id']\n",
    "\n",
    "    user_index = users_hash[user]\n",
    "    bu = b_u_hat[user_index][0]\n",
    "\n",
    "    song_index = songs_hash[song]\n",
    "    bi = b_i_hat[song_index-num_users][0]\n",
    "    test_pred_1.append(avg + bu + bi)\n",
    "test_pred_1 = np.maximum(test_pred_1,0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## part 1 - predicated!!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create the LS matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "num_records = df.shape[0]\n",
    "A = sp.lil_matrix((num_records, num_users + num_songs))\n",
    "c = np.full(num_records, -r_avg)\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # Set the nonzero entries in the current row\n",
    "    user = users_hash[row['user_id']]\n",
    "    song = songs_hash[row['song_id']]\n",
    "    A[i, user] = 1\n",
    "    A[i, song] = 1\n",
    "    c[i] += row['weight']\n",
    "\n",
    "A = A.tocsc()\n",
    "c = c.reshape((len(c), 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## perform gradient descent for LS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048561.1958076822\n",
      "1\n",
      "1024865.6997130646\n",
      "2\n",
      "1020372.3372163313\n",
      "3\n",
      "1018647.4560300034\n",
      "4\n",
      "1017696.1308030526\n",
      "5\n",
      "1017088.8805048653\n",
      "6\n",
      "1016673.0226228106\n",
      "7\n",
      "1016375.899287761\n",
      "8\n",
      "1016157.3831916937\n",
      "9\n",
      "1015993.2366633832\n",
      "10\n",
      "1015867.8703221263\n",
      "11\n",
      "1015770.8184313382\n",
      "12\n",
      "1015694.8143090488\n",
      "13\n",
      "1015634.6930668377\n",
      "14\n",
      "1015586.7069594894\n",
      "15\n",
      "1015548.0964226071\n",
      "16\n",
      "1015516.79964296\n",
      "17\n",
      "1015491.2599618526\n",
      "18\n",
      "1015470.2885258462\n",
      "19\n",
      "1015452.9698801278\n",
      "20\n",
      "1015438.5922028866\n",
      "21\n",
      "1015426.5981629011\n",
      "22\n",
      "1015416.5476313138\n",
      "23\n",
      "1015408.0909296651\n",
      "24\n",
      "1015400.948098525\n",
      "25\n",
      "1015394.8937984004\n",
      "26\n",
      "1015389.7453976574\n",
      "27\n",
      "1015385.3541804826\n",
      "28\n",
      "1015381.5982996996\n",
      "29\n",
      "1015378.3775109202\n",
      "30\n",
      "1015375.6088901877\n",
      "31\n",
      "1015373.223596674\n",
      "32\n",
      "1015371.164204503\n",
      "33\n",
      "1015369.3826635092\n",
      "34\n",
      "1015367.8385974952\n",
      "35\n",
      "1015366.4979903366\n",
      "36\n",
      "1015365.3320768808\n",
      "37\n",
      "1015364.3164786713\n",
      "38\n",
      "1015363.4304666503\n",
      "39\n",
      "1015362.6563818681\n",
      "40\n",
      "1015361.9791365175\n",
      "41\n",
      "1015361.3858190784\n",
      "42\n",
      "1015360.8653512181\n",
      "43\n",
      "1015360.4082146139\n",
      "44\n",
      "1015360.0062116743\n",
      "45\n",
      "1015359.6522740513\n",
      "46\n",
      "1015359.3402936801\n",
      "47\n",
      "1015359.064987\n",
      "48\n",
      "1015358.8217743301\n",
      "49\n",
      "1015358.6066826143\n",
      "50\n",
      "1015358.4162584784\n",
      "51\n",
      "1015358.2474979365\n",
      "52\n",
      "1015358.097783186\n",
      "53\n",
      "1015357.9648314061\n",
      "54\n",
      "1015357.8466484533\n",
      "55\n",
      "1015357.7414913\n",
      "56\n",
      "1015357.6478338813\n",
      "57\n",
      "1015357.5643393665\n",
      "58\n",
      "1015357.4898348079\n",
      "59\n",
      "1015357.4232905502\n",
      "60\n",
      "1015357.3638013119\n",
      "61\n",
      "1015357.310570811\n",
      "62\n",
      "1015357.2628975697\n",
      "63\n",
      "1015357.2201633835\n",
      "64\n",
      "1015357.1818226147\n",
      "65\n",
      "1015357.1473935047\n",
      "66\n",
      "1015357.1164500631\n",
      "67\n",
      "1015357.088615492\n",
      "68\n",
      "1015357.0635560175\n",
      "69\n",
      "1015357.0409758912\n",
      "70\n",
      "1015357.020612677\n",
      "71\n",
      "1015357.0022334395\n",
      "72\n",
      "1015356.9856311311\n",
      "73\n",
      "1015356.9706216697\n",
      "74\n",
      "1015356.9570411586\n",
      "75\n",
      "1015356.9447436434\n",
      "76\n",
      "1015356.933598962\n",
      "77\n",
      "1015356.9234910152\n",
      "78\n",
      "1015356.9143161\n",
      "79\n",
      "1015356.9059815725\n",
      "80\n",
      "1015356.8984045489\n",
      "81\n",
      "1015356.8915108703\n",
      "82\n",
      "1015356.8852340893\n",
      "83\n",
      "1015356.8795146602\n",
      "84\n",
      "1015356.8742991498\n",
      "85\n",
      "1015356.8695396021\n",
      "86\n",
      "1015356.865192918\n",
      "87\n",
      "1015356.8612203627\n",
      "88\n",
      "1015356.85758707\n",
      "89\n",
      "1015356.854261659\n",
      "90\n",
      "1015356.8512158428\n",
      "91\n",
      "1015356.8484241242\n",
      "92\n",
      "1015356.8458634889\n",
      "93\n",
      "1015356.8435131614\n",
      "94\n",
      "1015356.8413543624\n",
      "95\n",
      "1015356.8393701174\n",
      "96\n",
      "1015356.83754506\n",
      "97\n",
      "1015356.8358652815\n",
      "98\n",
      "1015356.8343181749\n",
      "99\n",
      "1015356.8328923122\n",
      "100\n",
      "1015356.831577321\n",
      "101\n",
      "1015356.8303637885\n",
      "102\n",
      "1015356.8292431571\n",
      "103\n",
      "1015356.8282076501\n",
      "104\n",
      "1015356.827250192\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "def step_size(AXC):\n",
    "    step_numerator = (la.norm(A.transpose().dot(AXC))) ** 3\n",
    "    step_denominator = (la.norm(A.dot(A.transpose().dot(AXC)))) ** 2\n",
    "    return step_numerator / step_denominator\n",
    "\n",
    "\n",
    "def grad(AXC):\n",
    "\n",
    "    grad_numerator = A.transpose().dot(AXC)\n",
    "    grad_denominator = la.norm(grad_numerator)\n",
    "    return grad_numerator / grad_denominator\n",
    "\n",
    "\n",
    "# gradient descent least squares:\n",
    "max_iterations = 10**6\n",
    "iteration = 0\n",
    "epsilon = 0.001\n",
    "gradient = 2 * epsilon\n",
    "x0 = np.random.random((A.shape[1], 1))\n",
    "x_new = x0\n",
    "f_vals = [np.inf]\n",
    "first = True\n",
    "while first or (abs(la.norm(f_vals[-1])-la.norm(f_vals[-2])) >= epsilon and iteration<=max_iterations):\n",
    "    first = False\n",
    "    x_prev = x_new\n",
    "    AX = A.dot(x_prev)\n",
    "    AXC = AX - c\n",
    "    f_vals.append(la.norm(AXC))\n",
    "    gradient = grad(AXC)\n",
    "    # print(la.norm(AXC))\n",
    "    step = step_size(AXC)\n",
    "    x_new = x_prev - (step * gradient)\n",
    "    iteration += 1\n",
    "    # print(iteration)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 2 - ALS Method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# user dictionary\n",
    "users_hash = {}\n",
    "for i,user in enumerate(all_users):\n",
    "    users_hash[user] = i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# song dictionary\n",
    "songs_hash = {}\n",
    "for j,song in enumerate(all_songs):\n",
    "    songs_hash[song] = j"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(1892, 16548)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = len(all_users)  # Number of users\n",
    "num_songs = len(all_songs)  # Number of songs\n",
    "num_users, num_songs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create the ratings matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "mat_part2 = true_mat.tolil(copy=True).astype(float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "for idx in test_indxs_mapped:\n",
    "    mat_part2[idx] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "mat_part2_rows =mat_part2.tocsr(copy=True)\n",
    "mat_part2_cols= mat_part2.tocsc(copy=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16548,)\n"
     ]
    }
   ],
   "source": [
    "print(mat_part2_rows.getrow(0).toarray().flatten().shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "# alternating least squares step\n",
    "def als_step(latent_vectors, fixed_vecs, ratings, _lambda=0, type='user'):\n",
    "    \"\"\"\n",
    "    One of the two ALS steps. Solve for the latent vectors\n",
    "    specified by type.\n",
    "    \"\"\"\n",
    "    if type == 'user':\n",
    "        # calculating P, Q is fixed\n",
    "\n",
    "        QTQ = fixed_vecs.T.dot(fixed_vecs)\n",
    "        lambdaI = np.eye(QTQ.shape[0]) * _lambda\n",
    "\n",
    "        for u in range(latent_vectors.shape[0]):  # iterate over P rows and R Rows\n",
    "            r = ratings.getrow(u).toarray().flatten() # deep copy?\n",
    "            rows_to_remove = np.where(np.isnan(r))[0]\n",
    "            clean_r = r[~np.isnan(r)]\n",
    "            mat = np.delete(fixed_vecs,rows_to_remove,axis=0)\n",
    "            new_vec = la.lstsq(mat,clean_r,rcond=None)[0]\n",
    "            # if u==0:\n",
    "                # print(f\"prev vec user {u}: {latent_vectors[u,:]}\")\n",
    "            # new_vec = (la.solve((QTQ + lambdaI),\n",
    "            #                                  (ratings[u, :].dot(fixed_vecs)).transpose())).ravel()\n",
    "            # if u==0:\n",
    "                # print(new_vec)\n",
    "            latent_vectors[u, :] = new_vec.ravel()\n",
    "                # print(f\"{np.array_equal(latent_vectors[u,:],new_vec)}\")\n",
    "    elif type == 'item':\n",
    "        # calculating Q, P is fixed\n",
    "        PTP = fixed_vecs.T.dot(fixed_vecs)\n",
    "        lambdaI = np.eye(PTP.shape[0]) * _lambda\n",
    "\n",
    "        for i in range(latent_vectors.shape[0]):\n",
    "            # remove rows from\n",
    "            r = ratings.getcol(i).toarray()\n",
    "            rows_to_remove = np.where(np.isnan(r))[0]\n",
    "            clean_r = r[~np.isnan(r)]\n",
    "            mat = np.delete(fixed_vecs,rows_to_remove,axis=0)\n",
    "            new_vec = la.lstsq(mat,clean_r,rcond=None)[0]\n",
    "            # if i ==0:\n",
    "            #     print(f\"prev vec song {i}: {latent_vectors[i,:]}\")\n",
    "            # new_vec =  (la.solve((PTP + lambdaI),\n",
    "            #                                  (ratings[:, i].transpose().dot(fixed_vecs)).transpose())).ravel()\n",
    "            # # if i==0:\n",
    "                # print(new_vec)\n",
    "            latent_vectors[i, :] = new_vec.ravel()\n",
    "                # print(f\"{np.array_equal(latent_vectors[i,:],new_vec)}\")\n",
    "    return latent_vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "def als_predict_vecs(checkpoints, iterations = 10, k = 20,learning_curve=False):\n",
    "    np.random.seed(0)\n",
    "    new_users = np.random.random((num_users, k))  # TODO: change to ones, but it throws an error\n",
    "    new_songs = np.zeros((num_songs, k))\n",
    "    scores_arr = []\n",
    "    for k in range(iterations): #or not (np.array_equal(old_songs, new_songs) and np.array_equal(old_users,\n",
    "        # new_users)):\n",
    "        new_songs = als_step(new_songs,\n",
    "                               new_users,\n",
    "                               mat_part2_cols,\n",
    "                               type='item')\n",
    "        new_users = als_step(new_users,\n",
    "                               new_songs,\n",
    "                               mat_part2_rows,\n",
    "                               type='user')\n",
    "        if learning_curve:\n",
    "            if k in checkpoints:\n",
    "                meantime_pred = predict(new_users,new_songs)\n",
    "                meantime_pred = np.maximum(meantime_pred,0)\n",
    "                scores_arr.append(get_score(true_mat,meantime_pred,N))\n",
    "    return new_users, new_songs, scores_arr\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Prediction - part 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "# user_matrix,song_matrix = als_predict()\n",
    "def predict(users, songs):\n",
    "    return np.matmul(users, songs.T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "def run_part_2(iterations):\n",
    "    user_vecs, song_vecs, scores_arr = als_predict_vecs(checkpoints=[2,5,10,15,19],iterations=iterations,\n",
    "                                                        learning_curve=True)\n",
    "    print(f'{iterations} iterations')\n",
    "    return scores_arr\n",
    "    # predictions_p2 = predict(user_vecs, song_vecs)\n",
    "    # predictions_p2 = np.maximum(predictions_p2,0)\n",
    "    # return get_score(true_mat,predictions_p2,N),user_vecs, song_vecs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## check the learning curve"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-224-73707e05017b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mK\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mscores_part2\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0muser_matrix\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0msong_matrix\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrun_part_2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterations\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# TODO: 10?, 2 works better though\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m print(f'RMSE in Part 2 on train set: {scores_part2[\"RMSE\"]}\\n'\n\u001B[0;32m      4\u001B[0m       \u001B[1;34mf'SSE in Part 2 on train set: {scores_part2[\"SSE\"]}\\n'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m       \u001B[1;34mf'compared to part 1\\n'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-223-ee9d7be1fd05>\u001B[0m in \u001B[0;36mrun_part_2\u001B[1;34m(iterations)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mrun_part_2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterations\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m     user_vecs, song_vecs, scores_arr = als_predict_vecs(checkpoints=[2,5,10,15,19],iterations=iterations,\n\u001B[0m\u001B[0;32m      3\u001B[0m                                                         learning_curve=True)\n\u001B[0;32m      4\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'{iterations} iterations'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mscores_arr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-222-fdc6b0b87820>\u001B[0m in \u001B[0;36mals_predict_vecs\u001B[1;34m(checkpoints, iterations, k, learning_curve)\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mk\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterations\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;31m#or not (np.array_equal(old_songs, new_songs) and np.array_equal(old_users,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[1;31m# new_users)):\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         new_songs = als_step(new_songs,\n\u001B[0m\u001B[0;32m      9\u001B[0m                                \u001B[0mnew_users\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m                                \u001B[0mmat_part2_cols\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-213-c42d3da2de12>\u001B[0m in \u001B[0;36mals_step\u001B[1;34m(latent_vectors, fixed_vecs, ratings, _lambda, type)\u001B[0m\n\u001B[0;32m     33\u001B[0m             \u001B[1;31m# remove rows from\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m             \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mratings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetcol\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m             \u001B[0mrows_to_remove\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwhere\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misnan\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m             \u001B[0mclean_r\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mr\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m~\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misnan\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m             \u001B[0mmat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdelete\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfixed_vecs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mrows_to_remove\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001B[0m in \u001B[0;36mwhere\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "K=20\n",
    "scores_part2,user_matrix,song_matrix = run_part_2(iterations=20) # TODO: 10?, 2 works better though\n",
    "print(f'RMSE in Part 2 on train set: {scores_part2[\"RMSE\"]}\\n'\n",
    "      f'SSE in Part 2 on train set: {scores_part2[\"SSE\"]}\\n'\n",
    "      f'compared to part 1\\n'\n",
    "      f'Improvement of {scores_part1[\"RMSE\"] - scores_part2[\"RMSE\"]} in the RMSE\\n'\n",
    "      f'Improvement of {scores_part1[\"SSE\"] - scores_part2[\"SSE\"]} in the SSE')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 iterations\n"
     ]
    }
   ],
   "source": [
    "scores_test = run_part_2(iterations=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RMSE_curve = [d['RMSE'] for d in scores_test]\n",
    "SSE_curve = [d['SSE'] for d in scores_test]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Prediction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_user_entry = user_matrix.mean()\n",
    "median_user_entry = np.median(user_matrix)\n",
    "\n",
    "mean_song_entry = song_matrix.mean()\n",
    "median_song_entry = np.median(song_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "test_pred_2 = []\n",
    "for i, row in test_df.iterrows():\n",
    "    user = row['user_id']\n",
    "    song = row['song_id']\n",
    "    user_index = users_hash[user]\n",
    "    user_vec = user_matrix[user_index]\n",
    "    song_index = songs_hash[song]\n",
    "    song_vec = song_matrix[song_index]\n",
    "    test_pred_2.append(max(user_vec.dot(song_vec),0))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## predicated"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "# create the sparse ratings matrix\n",
    "no_zeros = pd.read_csv('user_song.csv')\n",
    "R = sp.lil_matrix((num_users, num_songs))\n",
    "\n",
    "for i, row in no_zeros.iterrows():\n",
    "    # Set the nonzero entries in the current row\n",
    "    user = users_hash[row['user_id']]\n",
    "    song = songs_hash[row['song_id']]\n",
    "    rating = row['weight']\n",
    "    R[user, song] = rating\n",
    "\n",
    "R_rows = R.tocsr()  # we'll use to slice rows\n",
    "R_cols = R.tocsc()  # we'll use to slice column"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "494270.1826472662"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform SVD on the sparse matrix\n",
    "# k = Number of singular values/vectors to use\n",
    "U, S, V = svds(R_rows, k=20)\n",
    "\n",
    "# Reconstruct the matrix using k singular values/vectors\n",
    "reduced_R = U.dot(sp.diags(S).dot(V))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## part 3 train SSE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "244303013454.1619"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la.norm(R - reduced_R)**2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "part 3 test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "test_pred_3 = []\n",
    "for i, row in test_df.iterrows():\n",
    "    user = row['user_id']\n",
    "    song = row['song_id']\n",
    "    user_index = users_hash[user]\n",
    "    song_index = songs_hash[song]\n",
    "    test_pred_3.append(max(reduced_R[user_index,song_index],0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ??"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.61986513e+02,  8.16646444e-01,  4.15984691e+01, ...,\n         1.74729460e+01, -2.21394839e+01,  1.45697821e+01],\n       [ 9.83777088e-01,  1.04461350e-01,  2.58215767e-01, ...,\n        -2.58216215e-01,  7.63471391e-01,  1.75510303e+00],\n       [ 1.72788891e+03,  2.87772251e+00,  5.79312401e+01, ...,\n        -5.16097506e+00,  1.75152615e+02, -2.76223694e+01],\n       ...,\n       [ 4.91562118e+00,  1.20800706e+01,  2.17293498e+03, ...,\n        -1.99362494e+02, -4.39169951e+02,  1.83718364e+03],\n       [ 2.32986067e+00,  1.15089461e+01,  5.04095525e+02, ...,\n         2.63574394e+01,  2.70538707e+02, -5.24158997e+01],\n       [ 5.74155471e+00,  3.02752030e+01,  1.36511058e+03, ...,\n        -4.46342309e+01,  2.22556927e+02,  2.02332855e+02]])"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "movie_svd = TruncatedSVD(n_components=20)\n",
    "movie_features = movie_svd.fit_transform(R_rows)\n",
    "movie_features\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]\n",
      " [2. 2. 2. 2.]\n",
      " [3. 3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector = np.array([1, 2, 3])\n",
    "n = vector.shape[0]\n",
    "m = 4 # Desired number of columns in the resulting matrix\n",
    "\n",
    "matrix = vector.reshape(n, 1) * np.ones((1, m))\n",
    "\n",
    "print(matrix)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "3"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(3).shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}